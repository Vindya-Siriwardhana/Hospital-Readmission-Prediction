{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospital Readmission Prediction Project\n",
    "## Phase 6-7: Model Building & Evaluation\n",
    "\n",
    "**Author:** Vindya Siriwardhana  \n",
    "**Previous Phase Results:**\n",
    "- Features: 26 engineered features\n",
    "- Training: Balanced with SMOTE (1:1)\n",
    "- Test: Original distribution (11.16% readmission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## SETUP & LOAD PREPARED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report, precision_recall_curve\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load prepared data from Phase 4-5\n",
    "with open('/home/claude/hospital_readmission_prepared_data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data['X_train']\n",
    "X_test = data['X_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']\n",
    "feature_names = data['feature_names']\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\")\n",
    "print(f\"\\nðŸ“Š Training set: {X_train.shape}\")\n",
    "print(f\"ðŸ“Š Test set: {X_test.shape}\")\n",
    "print(f\"ðŸ“Š Features: {len(feature_names)}\")\n",
    "print(f\"\\nðŸŽ¯ Training balance: {pd.Series(y_train).value_counts()[1]} : {pd.Series(y_train).value_counts()[0]}\")\n",
    "print(f\"ðŸŽ¯ Test readmission rate: {y_test.mean()*100:.2f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PHASE 6: MODEL BUILDING (Steps 16-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Train Baseline Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 16: LOGISTIC REGRESSION (BASELINE MODEL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train Logistic Regression\n",
    "print(\"\\nðŸ”§ Training Logistic Regression...\")\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Model trained!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_auc = roc_auc_score(y_test, y_pred_proba_lr)\n",
    "\n",
    "print(\"\\nðŸ“Š LOGISTIC REGRESSION RESULTS:\")\n",
    "print(f\"   Accuracy:  {lr_accuracy:.4f}\")\n",
    "print(f\"   Precision: {lr_precision:.4f}\")\n",
    "print(f\"   Recall:    {lr_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {lr_auc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 17: RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nðŸ”§ Training Random Forest...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Model trained!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "rf_auc = roc_auc_score(y_test, y_pred_proba_rf)\n",
    "\n",
    "print(\"\\nðŸ“Š RANDOM FOREST RESULTS:\")\n",
    "print(f\"   Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"   Precision: {rf_precision:.4f}\")\n",
    "print(f\"   Recall:    {rf_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {rf_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {rf_auc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 18: Train XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 18: XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\nðŸ”§ Training XGBoost...\")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"âœ… Model trained!\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "xgb_precision = precision_score(y_test, y_pred_xgb)\n",
    "xgb_recall = recall_score(y_test, y_pred_xgb)\n",
    "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
    "xgb_auc = roc_auc_score(y_test, y_pred_proba_xgb)\n",
    "\n",
    "print(\"\\nðŸ“Š XGBOOST RESULTS:\")\n",
    "print(f\"   Accuracy:  {xgb_accuracy:.4f}\")\n",
    "print(f\"   Precision: {xgb_precision:.4f}\")\n",
    "print(f\"   Recall:    {xgb_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {xgb_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {xgb_auc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 19: Hyperparameter Tuning (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 19: HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compare models to select best for tuning\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n",
    "    'Accuracy': [lr_accuracy, rf_accuracy, xgb_accuracy],\n",
    "    'Precision': [lr_precision, rf_precision, xgb_precision],\n",
    "    'Recall': [lr_recall, rf_recall, xgb_recall],\n",
    "    'F1-Score': [lr_f1, rf_f1, xgb_f1],\n",
    "    'AUC-ROC': [lr_auc, rf_auc, xgb_auc]\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š Model Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Select best model based on AUC-ROC\n",
    "best_model_idx = results_df['AUC-ROC'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "print(f\"\\nðŸ† Best model by AUC-ROC: {best_model_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Hyperparameter tuning for best model (assuming XGBoost or RF)\n",
    "print(f\"\\nðŸ”§ Tuning hyperparameters for {best_model_name}...\")\n",
    "print(\"â³ This may take 5-10 minutes...\")\n",
    "\n",
    "if 'XGBoost' in best_model_name:\n",
    "    # XGBoost tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "elif 'Random Forest' in best_model_name:\n",
    "    # Random Forest tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "else:\n",
    "    # Logistic Regression tuning\n",
    "    param_grid = {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        param_grid,\n",
    "        cv=3,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nâœ… Hyperparameter tuning complete!\")\n",
    "print(f\"\\nðŸŽ¯ Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"ðŸŽ¯ Best cross-validation AUC-ROC: {grid_search.best_score_:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Predictions with tuned model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "best_precision = precision_score(y_test, y_pred_best)\n",
    "best_recall = recall_score(y_test, y_pred_best)\n",
    "best_f1 = f1_score(y_test, y_pred_best)\n",
    "best_auc = roc_auc_score(y_test, y_pred_proba_best)\n",
    "\n",
    "print(\"\\nðŸ“Š TUNED MODEL RESULTS:\")\n",
    "print(f\"   Accuracy:  {best_accuracy:.4f}\")\n",
    "print(f\"   Precision: {best_precision:.4f}\")\n",
    "print(f\"   Recall:    {best_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {best_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {best_auc:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PHASE 7: MODEL EVALUATION (Steps 20-22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 20: Calculate All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 20: COMPREHENSIVE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final comparison with tuned model\n",
    "final_results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost', f'{best_model_name} (Tuned)'],\n",
    "    'Accuracy': [lr_accuracy, rf_accuracy, xgb_accuracy, best_accuracy],\n",
    "    'Precision': [lr_precision, rf_precision, xgb_precision, best_precision],\n",
    "    'Recall': [lr_recall, rf_recall, xgb_recall, best_recall],\n",
    "    'F1-Score': [lr_f1, rf_f1, xgb_f1, best_f1],\n",
    "    'AUC-ROC': [lr_auc, rf_auc, xgb_auc, best_auc]\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š FINAL MODEL COMPARISON:\")\n",
    "print(final_results.to_string(index=False))\n",
    "\n",
    "# Highlight best scores\n",
    "print(\"\\nðŸ† BEST SCORES:\")\n",
    "for col in ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']:\n",
    "    best_idx = final_results[col].idxmax()\n",
    "    best_val = final_results[col].max()\n",
    "    best_mod = final_results.loc[best_idx, 'Model']\n",
    "    print(f\"   {col:12s}: {best_val:.4f} ({best_mod})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize metrics comparison\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "final_results_plot = final_results.set_index('Model')[metrics_to_plot]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "final_results_plot.plot(kind='bar', ax=ax, width=0.8)\n",
    "ax.set_title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "ax.set_xlabel('Model', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 21: Confusion Matrices & ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 21: CONFUSION MATRICES & ROC CURVES\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "models_data = [\n",
    "    ('Logistic Regression', y_pred_lr),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('XGBoost', y_pred_xgb),\n",
    "    (f'{best_model_name} (Tuned)', y_pred_best)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(models_data):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[row, col],\n",
    "                xticklabels=['Not Readmit', 'Readmit'],\n",
    "                yticklabels=['Not Readmit', 'Readmit'])\n",
    "    axes[row, col].set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Predicted')\n",
    "    axes[row, col].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Confusion matrices plotted!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROC Curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC={lr_auc:.4f})', linewidth=2)\n",
    "\n",
    "# Random Forest\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={rf_auc:.4f})', linewidth=2)\n",
    "\n",
    "# XGBoost\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC={xgb_auc:.4f})', linewidth=2)\n",
    "\n",
    "# Tuned model\n",
    "fpr_best, tpr_best, _ = roc_curve(y_test, y_pred_proba_best)\n",
    "plt.plot(fpr_best, tpr_best, label=f'{best_model_name} Tuned (AUC={best_auc:.4f})', linewidth=2, linestyle='--')\n",
    "\n",
    "# Random classifier line\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - All Models', fontsize=16, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… ROC curves plotted!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 22: Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 22: SELECT BEST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select best model (tuned model)\n",
    "final_model = best_model\n",
    "\n",
    "print(f\"\\nðŸ† FINAL SELECTED MODEL: {best_model_name} (Tuned)\")\n",
    "print(\"\\nðŸ“Š FINAL MODEL PERFORMANCE:\")\n",
    "print(f\"   Accuracy:  {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "print(f\"   Precision: {best_precision:.4f} ({best_precision*100:.2f}%)\")\n",
    "print(f\"   Recall:    {best_recall:.4f} ({best_recall*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {best_f1:.4f}\")\n",
    "print(f\"   AUC-ROC:   {best_auc:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ INTERPRETATION:\")\n",
    "print(f\"   â€¢ Model correctly identifies {best_accuracy*100:.1f}% of all patients\")\n",
    "print(f\"   â€¢ When model predicts readmission, it's correct {best_precision*100:.1f}% of the time\")\n",
    "print(f\"   â€¢ Model catches {best_recall*100:.1f}% of actual readmissions\")\n",
    "print(f\"   â€¢ AUC-ROC of {best_auc:.3f} indicates {'excellent' if best_auc > 0.8 else 'good' if best_auc > 0.7 else 'fair'} discrimination\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Classification report\n",
    "print(\"\\nðŸ“‹ DETAILED CLASSIFICATION REPORT:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Not Readmitted', 'Readmitted <30']))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance (if tree-based model)\n",
    "if hasattr(final_model, 'feature_importances_'):\n",
    "    print(\"\\nðŸ“Š TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': final_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Importance', fontsize=12)\n",
    "    plt.title('Top 15 Feature Importances', fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Feature importance analyzed!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ’¾ SAVE FINAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING FINAL MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save model and results\n",
    "model_artifacts = {\n",
    "    'model': final_model,\n",
    "    'model_name': best_model_name,\n",
    "    'feature_names': feature_names,\n",
    "    'scaler': data['scaler'],\n",
    "    'label_encoders': data['label_encoders'],\n",
    "    'metrics': {\n",
    "        'accuracy': best_accuracy,\n",
    "        'precision': best_precision,\n",
    "        'recall': best_recall,\n",
    "        'f1': best_f1,\n",
    "        'auc': best_auc\n",
    "    },\n",
    "    'best_params': grid_search.best_params_\n",
    "}\n",
    "\n",
    "with open('/home/claude/hospital_readmission_final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(\"\\nðŸ’¾ Saved:\")\n",
    "print(\"   âœ“ Final trained model\")\n",
    "print(\"   âœ“ Feature names\")\n",
    "print(\"   âœ“ Scaler & encoders\")\n",
    "print(\"   âœ“ Performance metrics\")\n",
    "print(\"   âœ“ Best hyperparameters\")\n",
    "\n",
    "print(\"\\nðŸ“ File: hospital_readmission_final_model.pkl\")\n",
    "print(\"\\nâœ… Model ready for deployment!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸ“Š PHASE 6-7 SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 6-7 COMPLETED SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nâœ… COMPLETED STEPS:\")\n",
    "print(\"   Step 16: âœ“ Logistic Regression (baseline)\")\n",
    "print(\"   Step 17: âœ“ Random Forest\")\n",
    "print(\"   Step 18: âœ“ XGBoost\")\n",
    "print(\"   Step 19: âœ“ Hyperparameter tuning (GridSearchCV)\")\n",
    "print(\"   Step 20: âœ“ Comprehensive metrics calculated\")\n",
    "print(\"   Step 21: âœ“ Confusion matrices & ROC curves\")\n",
    "print(\"   Step 22: âœ“ Best model selected & saved\")\n",
    "\n",
    "print(f\"\\nðŸ† FINAL MODEL: {best_model_name} (Tuned)\")\n",
    "print(f\"\\nðŸ“Š KEY METRICS:\")\n",
    "print(f\"   â€¢ AUC-ROC:   {best_auc:.4f}\")\n",
    "print(f\"   â€¢ Accuracy:  {best_accuracy:.4f}\")\n",
    "print(f\"   â€¢ Precision: {best_precision:.4f}\")\n",
    "print(f\"   â€¢ Recall:    {best_recall:.4f}\")\n",
    "print(f\"   â€¢ F1-Score:  {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ NEXT STEPS (Phase 8):\")\n",
    "print(\"   Step 23: Feature importance analysis\")\n",
    "print(\"   Step 24: SHAP values implementation\")\n",
    "print(\"   Step 25: SHAP visualizations\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Ready to proceed to Phase 8: Explainability (SHAP)!\")\n",
    "print(\"=\"*80)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
